{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OliaDaX_lwou"
   },
   "source": [
    "# **ğŸ“„ Document type classification baseline code**\n",
    "> ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰     \n",
    "> ì•„ë˜ baselineì—ì„œëŠ” ResNet ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬, ëª¨ë¸ì„ í•™ìŠµ ë° ì˜ˆì¸¡ íŒŒì¼ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## Version Info\n",
    "- ëª¨ë¸ í•™ìŠµ ë°©ë²• : ViT\n",
    "- Augmentation + í•˜ì´í¼íŒŒë¼ë¯¸í„° + ê°€ì¤‘ì¹˜ + ìŠ¤ì¼€ì¤„ëŸ¬ + weight decay + TTA + í™€ë“œì•„ì›ƒ + ì˜¤í”„ë¼ì¸ ì¦ê°•\n",
    "\n",
    "## Contents\n",
    "- Prepare Environments\n",
    "- Import Library & Define Functions\n",
    "- Hyper-parameters\n",
    "- Load Data\n",
    "- Train Model\n",
    "- Inference & Save File\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zkH9T_86lDSS"
   },
   "source": [
    "## 1. Prepare Environments\n",
    "\n",
    "* ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•œ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤.\n",
    "* í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21945,
     "status": "ok",
     "timestamp": 1700314517484,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "pUjnEto4gIZm",
    "outputId": "0999f10c-e1ff-428c-995b-481eec8a0b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸, Colabì„ ì´ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ íŒ¨ìŠ¤í•´ë„ ë©ë‹ˆë‹¤.\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7640,
     "status": "ok",
     "timestamp": 1700314537985,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "5lFQ-gpjnN_m"
   },
   "outputs": [],
   "source": [
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œëœ ëŒ€íšŒ ë°ì´í„°ë¥¼ ì••ì¶• í•´ì œí•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "!tar -xvf drive/MyDrive/datasets_fin.tar > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8489,
     "status": "ok",
     "timestamp": 1700314558888,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "NC8V-D393wY4",
    "outputId": "e9927325-26c4-4b89-9c51-c1d6541388d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.12)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "!pip install timm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = 'datasets_fin/'\n",
    "\n",
    "# model config\n",
    "model_name = 'convnext_base' # 'efficientnet_b1' #'resnet50' 'resnet34' # , ...\n",
    "\n",
    "# training config - ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŒ \n",
    "img_size = 224\n",
    "LR = 3e-5\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "num_workers = 2 # ì†ë„ í–¥ìƒì„ ìœ„í•´ ì¡°ì •\n",
    "N_SPLITS = 5  # 5-Fold êµì°¨ ê²€ì¦\n",
    "EARLY_STOPPING_PATIENCE = 10 # 5 ì—í¬í¬ ë™ì•ˆ ì ìˆ˜ í–¥ìƒ ì—†ìœ¼ë©´ ì¤‘ë‹¨\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë°ì´í„°: 1570, K-Fold í•™ìŠµìš©: 1256, ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œìš©: 314\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ì›ë³¸ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "all_df = pd.read_csv(\"datasets_fin/train.csv\")\n",
    "\n",
    "# [âœ… í•µì‹¬] 1570ê°œë¥¼ 8:2 ë¹„ìœ¨ë¡œ 'í•™ìŠµìš©'ê³¼ 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œìš©'ìœ¼ë¡œ ë¶„ë¦¬\n",
    "# stratify=all_df['target'] : 17ê°œ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ì„ì–´ì¤Œ (Macro F1 ë¬¸ì œ í•´ê²°!)\n",
    "train_df, holdout_df = train_test_split(\n",
    "    all_df, \n",
    "    test_size=0.2,    # 20%ë¥¼ ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ(holdout)ë¡œ ì‚¬ìš©\n",
    "    random_state=42,  # í•­ìƒ ë™ì¼í•˜ê²Œ ë¶„ë¦¬\n",
    "    stratify=all_df['target'] \n",
    ")\n",
    "\n",
    "# í•™ìŠµì€ 80% ë°ì´í„°ë¡œë§Œ ì§„í–‰ (1570 * 0.8 = ì•½ 1256ê°œ)\n",
    "df = train_df.reset_index(drop=True) \n",
    "\n",
    "print(f\"ì´ ë°ì´í„°: {len(all_df)}, K-Fold í•™ìŠµìš©: {len(df)}, ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œìš©: {len(holdout_df)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤í”„ë¼ì¸ ë°ì´í„° ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1256/1256 [00:34<00:00, 36.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì¦ê°• ë° CSV ìƒì„± ì™„ë£Œ! (ì´ 6280ê°œ ì´ë¯¸ì§€)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ì˜¤í”„ë¼ì¸ ë°ì´í„° ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# ì›ë³¸ í´ë”ì™€ ì¦ê°• ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë”\n",
    "input_dir = \"datasets_fin/train\"\n",
    "output_dir = \"datasets_fin/train_aug\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ì¦ê°•ëœ íŒŒì¼ ëª©ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "aug_data = []\n",
    "\n",
    "# ì˜¤í”„ë¼ì¸ ì¦ê°•ì— ì‚¬ìš©í•  ê°„ë‹¨í•œ ë³€í™˜\n",
    "# (ì˜¨ë¼ì¸ ì¦ê°•ê³¼ëŠ” ë³„ê°œì…ë‹ˆë‹¤)\n",
    "augment = A.Compose([\n",
    "    A.Rotate(limit=180, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.GaussianBlur(p=0.5)\n",
    "])\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    img_name = row['ID']\n",
    "    label = row['target']\n",
    "    img_path = os.path.join(input_dir, img_name)\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        print(f\"ê²½ê³ : {img_path} íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        continue\n",
    "    \n",
    "    # 1. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ìƒˆ í´ë”ì— ê·¸ëŒ€ë¡œ ì €ì¥\n",
    "    cv2.imwrite(os.path.join(output_dir, img_name), image)\n",
    "    aug_data.append({\"ID\": img_name, \"target\": label})\n",
    "\n",
    "    # 2. ì¦ê°•ëœ ì´ë¯¸ì§€ 4ê°œ ìƒì„± ë° ì €ì¥\n",
    "    for i in range(4): # 4ë²ˆ ë°˜ë³µí•˜ì—¬ 4ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ ìƒì„±\n",
    "        augmented = augment(image=image)[\"image\"]\n",
    "        new_name = f\"{os.path.splitext(img_name)[0]}_aug{i+1}.jpg\"\n",
    "        cv2.imwrite(os.path.join(output_dir, new_name), augmented)\n",
    "        aug_data.append({\"ID\": new_name, \"target\": label})\n",
    "\n",
    "# ìƒˆ CSV íŒŒì¼ (aug_train.csv) ì €ì¥\n",
    "aug_df = pd.DataFrame(aug_data)\n",
    "aug_df.to_csv(\"datasets/aug_train.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ì¦ê°• ë° CSV ìƒì„± ì™„ë£Œ! (ì´ {len(aug_df)}ê°œ ì´ë¯¸ì§€)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ID  target\n",
      "0  186099f62d5cb389.jpg      10\n",
      "1  457cd32cb351d570.jpg      12\n",
      "2  591f6b87942dfe1d.jpg       7\n",
      "3  898368eb7ad07511.jpg       2\n",
      "4  d54f10f7792df3ec.jpg       7\n",
      "1256\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ID  target\n",
      "0       186099f62d5cb389.jpg      10\n",
      "1  186099f62d5cb389_aug1.jpg      10\n",
      "2  186099f62d5cb389_aug2.jpg      10\n",
      "3  186099f62d5cb389_aug3.jpg      10\n",
      "4  186099f62d5cb389_aug4.jpg      10\n",
      "6280\n"
     ]
    }
   ],
   "source": [
    "print(aug_df.head())\n",
    "print(len(aug_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "llh5C7ZKoq2S"
   },
   "outputs": [],
   "source": [
    "# augmentationì„ ìœ„í•œ transform ì½”ë“œ\n",
    "trn_transform = A.Compose([\n",
    "    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "\n",
    "    # ê·¹ì‹¬í•œ íšŒì „ ë° ì›ê·¼ ì™œê³¡ ì¶”ê°€\n",
    "    A.Perspective(scale=(0.05, 0.1), pad_mode=0, p=0.7),\n",
    "    A.Affine(\n",
    "        rotate=(-90, 90),       # -90ë„ ~ +90ë„ ì‚¬ì´ë¡œ ë§ˆêµ¬ íšŒì „\n",
    "        shear=(-15, 15),       # ì´ë¯¸ì§€ ê¸°ìš¸ì„\n",
    "        scale=(0.8, 1.2),      # 80%~120% í™•ëŒ€/ì¶•ì†Œ\n",
    "        translate_percent=0.1, # ìƒí•˜ì¢Œìš° ì´ë™\n",
    "        p=0.8\n",
    "    ),\n",
    "    # ì¡°ëª… ë° í’ˆì§ˆ ì €í•˜ í‰ë‚´ë‚´ê¸°\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.8),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "        A.MotionBlur(blur_limit=(3, 7), p=0.5),\n",
    "    ], p=0.5),\n",
    "    # ì´ë¯¸ì§€ ì¼ë¶€ ê°€ë¦¬ê¸° (ê³¼ì í•© ë°©ì§€)\n",
    "    A.CoarseDropout(max_holes=8, max_height=img_size//8, max_width=img_size//8, min_holes=1, p=0.7),\n",
    "    # ì¢Œìš° ë°˜ì „\n",
    "    # ìë™ì°¨ ì‚¬ì§„ë„ ìˆê³ , ë¬¸ì„œë„ ë’¤ì§‘í˜€ ìˆìœ¼ë¯€ë¡œ HorizontalFlipì€ ì¢‹ì€ ì¦ê°•ì…ë‹ˆë‹¤.\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "\n",
    "    # images normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # numpy ì´ë¯¸ì§€ë‚˜ PIL ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation í•¨ìˆ˜\n",
    "def validate_one_epoch(loader, model, loss_fn, device):\n",
    "    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™”\n",
    "        pbar = tqdm(loader, desc=\"Valid\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "            pbar.set_description(f\"Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro') # Macro F1\n",
    "\n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None, target_col='target'):\n",
    "        self.df = df \n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.df.iloc[idx]['ID']\n",
    "        target = self.df.iloc[idx][self.target_col]\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ì—´ê¸°\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        try:\n",
    "            img = np.array(Image.open(img_path))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Image file not found at {img_path}\")\n",
    "            # ë˜ëŠ” ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜\n",
    "            return torch.randn(3, img_size, img_size), 0 \n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## 5. Train Model\n",
    "* ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1700315114067,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "FbBgFPsLT-CO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' í•™ìŠµ ì‹œì‘ ---\n",
      "í•™ìŠµ ë°ì´í„°: 6280ê°œ (ì¦ê°•ë¨)\n",
      "ê²€ì¦ ë°ì´í„°: 314ê°œ (ê¹¨ë—í•œ ì›ë³¸)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2413: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:24<00:00,  9.34it/s]\n",
      "Valid Loss: 0.5770: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 30.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [85s] - Train F1: 0.7867, Valid F1 (Holdout): 0.8633, Valid Loss: 0.3394\n",
      "â­ï¸ Best F1 Score updated to 0.8633. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0404: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.55it/s]\n",
      "Valid Loss: 0.9735: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 39.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 [83s] - Train F1: 0.9024, Valid F1 (Holdout): 0.9136, Valid Loss: 0.2701\n",
      "â­ï¸ Best F1 Score updated to 0.9136. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0402: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.46it/s]\n",
      "Valid Loss: 0.4835: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 [84s] - Train F1: 0.9323, Valid F1 (Holdout): 0.8824, Valid Loss: 0.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1549: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.46it/s]\n",
      "Valid Loss: 0.0964: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 [84s] - Train F1: 0.9544, Valid F1 (Holdout): 0.9284, Valid Loss: 0.2162\n",
      "â­ï¸ Best F1 Score updated to 0.9284. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0251: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.48it/s]\n",
      "Valid Loss: 0.5981: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 [84s] - Train F1: 0.9634, Valid F1 (Holdout): 0.9020, Valid Loss: 0.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1324: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.47it/s]\n",
      "Valid Loss: 0.0175: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 39.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 [84s] - Train F1: 0.9698, Valid F1 (Holdout): 0.9050, Valid Loss: 0.3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.49it/s]\n",
      "Valid Loss: 0.0387: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 [84s] - Train F1: 0.9745, Valid F1 (Holdout): 0.9260, Valid Loss: 0.2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.47it/s]\n",
      "Valid Loss: 0.0300: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 [84s] - Train F1: 0.9759, Valid F1 (Holdout): 0.9161, Valid Loss: 0.2913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0147: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.47it/s]\n",
      "Valid Loss: 0.0595: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 [84s] - Train F1: 0.9840, Valid F1 (Holdout): 0.8919, Valid Loss: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.48it/s]\n",
      "Valid Loss: 0.0499: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 [84s] - Train F1: 0.9832, Valid F1 (Holdout): 0.9279, Valid Loss: 0.2615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0431: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.47it/s]\n",
      "Valid Loss: 0.0467: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 [84s] - Train F1: 0.9837, Valid F1 (Holdout): 0.9122, Valid Loss: 0.3304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.47it/s]\n",
      "Valid Loss: 0.1854: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 39.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 [84s] - Train F1: 0.9858, Valid F1 (Holdout): 0.8960, Valid Loss: 0.2930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0072: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.46it/s]\n",
      "Valid Loss: 0.0172: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 [84s] - Train F1: 0.9882, Valid F1 (Holdout): 0.9247, Valid Loss: 0.2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [01:22<00:00,  9.47it/s]\n",
      "Valid Loss: 0.0361: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:01<00:00, 39.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 [84s] - Train F1: 0.9908, Valid F1 (Holdout): 0.9057, Valid Loss: 0.4408\n",
      "Early stopping at epoch 14 as F1 score did not improve for 10 epochs.\n",
      "--- í•™ìŠµ ì™„ë£Œ ---\n",
      "ìµœì¢… 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' F1 ì ìˆ˜: 0.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Dataset ë° DataLoader ì •ì˜\n",
    "trn_dataset = ImageDataset(\n",
    "    aug_df,\n",
    "    \"datasets_fin/train_aug/\",\n",
    "    transform=trn_transform,\n",
    "    target_col='target'\n",
    ")\n",
    "val_dataset = ImageDataset(\n",
    "    holdout_df, \n",
    "    \"datasets_fin/train/\", \n",
    "    transform=val_transform,\n",
    "    target_col='target'\n",
    ")\n",
    "\n",
    "# DataLoader ì •ì˜\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "\n",
    "# Loss ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "current_num_classes = 17\n",
    "weights = torch.ones(current_num_classes, dtype=torch.float32).to(device)\n",
    "\n",
    "class_counts = train_df['target'].value_counts()\n",
    "\n",
    "if not class_counts.empty:\n",
    "    weight_values = 1. / torch.tensor(class_counts.values, dtype=torch.float32).to(device)\n",
    "    weights.scatter_(0, torch.tensor(class_counts.index, dtype=torch.long).to(device), weight_values)\n",
    "\n",
    "weights = (weights / weights.sum()) * current_num_classes \n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Optimizer, Scheduler\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Early Stoppingì„ ìœ„í•œ ë³€ìˆ˜\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"--- 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' í•™ìŠµ ì‹œì‘ ---\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(trn_dataset)}ê°œ (ì¦ê°•ë¨)\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ (ê¹¨ë—í•œ ì›ë³¸)\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)\n",
    "    \n",
    "    # [í•µì‹¬] 314ê°œì˜ ê¹¨ë—í•œ í™€ë“œì•„ì›ƒ ì…‹ìœ¼ë¡œ ê²€ì¦\n",
    "    val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "    \n",
    "    train_ret['epoch'] = epoch\n",
    "    elapsed = time.time() - start_time\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} [{elapsed:.0f}s] - \"\n",
    "          f\"Train F1: {train_ret['train_f1']:.4f}, \"\n",
    "          f\"Valid F1 (Holdout): {val_ret['val_f1']:.4f}, \" # <- ì´ê²Œ ì§„ì§œ ì ìˆ˜\n",
    "          f\"Valid Loss: {val_ret['val_loss']:.4f}\")\n",
    "\n",
    "    # Early Stopping ë° ëª¨ë¸ ì €ì¥ ë¡œì§\n",
    "    if val_ret['val_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_ret['val_f1']\n",
    "        \n",
    "        # [âœ… ìˆ˜ì •ë¨] ì €ì¥ ê²½ë¡œì—ì„œ {fold} ë³€ìˆ˜ ì œê±°\n",
    "        torch.save(model.state_dict(), f\"model/best_conv_model.pth\") \n",
    "        print(f\"â­ï¸ Best F1 Score updated to {best_val_f1:.4f}. Model saved!\")\n",
    "        patience_counter = 0 # ì´ˆê¸°í™”\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1} as F1 score did not improve for {EARLY_STOPPING_PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "print(f\"--- í•™ìŠµ ì™„ë£Œ ---\")\n",
    "print(f\"ìµœì¢… 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' F1 ì ìˆ˜: {best_val_f1:.4f}\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•˜ê³ , ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TTA ì ìš© 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' ì ìˆ˜ ê³„ì‚° ì‹œì‘ ---\n",
      "í™€ë“œì•„ì›ƒ ë°ì´í„°(holdout_df) ë¡œë“œ ì¤‘... (ì´ 314ê°œ)\n",
      "ëª¨ë¸ model/best_conv_model.pth ë¡œë“œ ì™„ë£Œ.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TTA ì ìš© 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' ìµœì¢… ì ìˆ˜ ---\n",
      "âœ… TTA ì ìš© Macro F1: 0.9253\n",
      "   (TTA ì ìš© Accuracy: 0.9299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import timm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- TTA ì ìš© 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' ì ìˆ˜ ê³„ì‚° ì‹œì‘ ---\n",
    "print(\"--- TTA ì ìš© 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' ì ìˆ˜ ê³„ì‚° ì‹œì‘ ---\")\n",
    "\n",
    "# (ì£¼ì˜: Cell 54ì—ì„œ ìƒì„±ëœ 'holdout_df' ë³€ìˆ˜ê°€ ì‚´ì•„ìˆì–´ì•¼ í•©ë‹ˆë‹¤)\n",
    "if 'holdout_df' not in globals():\n",
    "    print(\"ì˜¤ë¥˜: 'holdout_df'ê°€ ì—†ìŠµë‹ˆë‹¤. Load Data ì„¹í„°ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"í™€ë“œì•„ì›ƒ ë°ì´í„°(holdout_df) ë¡œë“œ ì¤‘... (ì´ {len(holdout_df)}ê°œ)\")\n",
    "\n",
    "    # 1. TTAìš© ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    # [âœ… í•µì‹¬] 'transform=tst_transform'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!\n",
    "    holdout_tta_dataset = ImageDataset(\n",
    "        holdout_df,               \n",
    "        \"datasets_fin/train/\",    # ê¹¨ë—í•œ ì›ë³¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” 'train' í´ë”\n",
    "        transform=tst_transform,  # TTAê°€ í¬í•¨ëœ 'tst_transform' ì‚¬ìš©\n",
    "        target_col='target'\n",
    "    )\n",
    "\n",
    "    holdout_tta_loader = DataLoader(\n",
    "        holdout_tta_dataset,\n",
    "        batch_size=BATCH_SIZE * 2, # ì¶”ë¡ ì€ ë°°ì¹˜ 2ë°°ë¡œ\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # 2. ì €ì¥ëœ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)\n",
    "    model_path = \"model/best_conv_model.pth\" # í•™ìŠµì—ì„œ ì €ì¥ëœ ê²½ë¡œ\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    print(f\"ëª¨ë¸ {model_path} ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "    # 3. TTA ì¶”ë¡  ì‹¤í–‰ (Inference ì„¹ì…˜ ì½”ë“œ ì¬í™œìš©)\n",
    "    tta_preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for image, _ in tqdm(holdout_tta_loader, desc=\"TTA ì¶”ë¡  ì¤‘\"):\n",
    "            image = image.to(device)\n",
    "            \n",
    "            # TTA 0: ì›ë³¸ ì˜ˆì¸¡\n",
    "            pred_orig = model(image)\n",
    "\n",
    "            # TTA 1: ì¢Œìš° ë°˜ì „\n",
    "            image_hflip = torch.flip(image, dims=[-1]) \n",
    "            pred_hflip = model(image_hflip)\n",
    "            \n",
    "            # TTA 2: 90ë„ íšŒì „\n",
    "            image_rot90 = torch.rot90(image, k=1, dims=[-2, -1])\n",
    "            pred_rot90 = model(image_rot90)\n",
    "\n",
    "            # 3ê°œ ì˜ˆì¸¡ì˜ í™•ë¥ (softmax)ì„ í‰ê· ëƒ„\n",
    "            avg_batch_pred = (pred_orig.softmax(dim=1) + \n",
    "                              pred_hflip.softmax(dim=1) + \n",
    "                              pred_rot90.softmax(dim=1)) / 3.0\n",
    "            \n",
    "            tta_preds_list.append(avg_batch_pred.cpu().numpy())\n",
    "    \n",
    "    # (314, 17) í¬ê¸°ì˜ ë°°ì—´ë¡œ í•©ì¹˜ê¸°\n",
    "    final_tta_preds_array = np.concatenate(tta_preds_list, axis=0)\n",
    "\n",
    "    # 4. TTA ì ìš© ì ìˆ˜ ê³„ì‚°\n",
    "    # ìµœì¢… ì˜ˆì¸¡ ë¼ë²¨ (ê°€ì¥ í™•ë¥  ë†’ì€ ê²ƒ)\n",
    "    final_tta_labels = np.argmax(final_tta_preds_array, axis=1)\n",
    "    \n",
    "    # ì‹¤ì œ ì •ë‹µ ë¼ë²¨\n",
    "    true_labels = holdout_df['target'].values\n",
    "    \n",
    "    # Macro F1 ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "    tta_f1_score = f1_score(true_labels, final_tta_labels, average='macro')\n",
    "    tta_acc_score = accuracy_score(true_labels, final_tta_labels)\n",
    "\n",
    "    print(\"\\n--- TTA ì ìš© 'ë¯¸ë‹ˆ ë¦¬ë”ë³´ë“œ' ìµœì¢… ì ìˆ˜ ---\")\n",
    "    print(f\"âœ… TTA ì ìš© Macro F1: {tta_f1_score:.4f}\")\n",
    "    print(f\"   (TTA ì ìš© Accuracy: {tta_acc_score:.4f})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ model/best_conv_model.pth ë¡œë“œ ì™„ë£Œ.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:25<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt í‰ê·  í™•ë¥  ì €ì¥ ì™„ë£Œ!\n",
      "ì¶”ë¡  ì™„ë£Œ. ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tst_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "tst_dataset = ImageDataset(\n",
    "    tst_df,\n",
    "    \"datasets_fin/test/\",\n",
    "    transform=tst_transform  # Test ì…‹ì—ëŠ” ì¦ê°•ì´ ì—†ëŠ” val_transform ì‚¬ìš©\n",
    ")\n",
    "\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 2. ì €ì¥ëœ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)\n",
    "model_path = \"model/best_conv_model.pth\" # í•™ìŠµì—ì„œ ì €ì¥ëœ ê²½ë¡œ\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(f\"ëª¨ë¸ {model_path} ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "# 3. TTA ì¶”ë¡  ì‹¤í–‰ (Inference ì„¹ì…˜ ì½”ë“œ ì¬í™œìš©)\n",
    "tta_preds_list = []\n",
    "with torch.no_grad():\n",
    "    for image, _ in tqdm(tst_loader, desc=\"TTA ì¶”ë¡  ì¤‘\"):\n",
    "        image = image.to(device)\n",
    "        \n",
    "        # TTA 0: ì›ë³¸ ì˜ˆì¸¡\n",
    "        pred_orig = model(image)\n",
    "\n",
    "        # TTA 1: ì¢Œìš° ë°˜ì „\n",
    "        image_hflip = torch.flip(image, dims=[-1]) \n",
    "        pred_hflip = model(image_hflip)\n",
    "        \n",
    "        # TTA 2: 90ë„ íšŒì „\n",
    "        image_rot90 = torch.rot90(image, k=1, dims=[-2, -1])\n",
    "        pred_rot90 = model(image_rot90)\n",
    "\n",
    "        # 3ê°œ ì˜ˆì¸¡ì˜ í™•ë¥ (softmax)ì„ í‰ê· ëƒ„\n",
    "        avg_batch_pred = (pred_orig.softmax(dim=1) + \n",
    "                            pred_hflip.softmax(dim=1) + \n",
    "                            pred_rot90.softmax(dim=1)) / 3.0\n",
    "        \n",
    "        tta_preds_list.append(avg_batch_pred.cpu().numpy())\n",
    "\n",
    "# 4. ìµœì¢… ì˜ˆì¸¡ ë¼ë²¨ ìƒì„±\n",
    "final_tta_preds_array = np.concatenate(tta_preds_list, axis=0)\n",
    "\n",
    "np.save(\"npy/conv_final_preds.npy\", final_tta_preds_array) \n",
    "print(\"ConvNeXt í‰ê·  í™•ë¥  ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "final_tta_labels = np.argmax(final_tta_preds_array, axis=1)\n",
    "\n",
    "# 5. [âœ… ìˆ˜ì •ë¨] F1 ìŠ¤ì½”ì–´ ê³„ì‚° ë¶€ë¶„ ***ì™„ì „ ì‚­ì œ***\n",
    "# (ìš°ë¦¬ëŠ” 'ì§„ì§œ í…ŒìŠ¤íŠ¸ ë°ì´í„°'ì˜ ì •ë‹µì„ ëª¨ë¥´ë¯€ë¡œ ê³„ì‚° ë¶ˆê°€ëŠ¥)\n",
    "print(\"ì¶”ë¡  ì™„ë£Œ. ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# 6. [âœ… ìˆ˜ì •ë¨] ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "pred_df = tst_df.copy()\n",
    "# [ì¤‘ìš”!] 'ê°€ì§œ ì •ë‹µ'(true_labels)ì´ ì•„ë‹Œ, 'ëª¨ë¸ì˜ ì˜ˆì¸¡'(final_tta_labels)ì„ ë„£ì–´ì•¼ í•¨\n",
    "pred_df['target'] = final_tta_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 12  5 ...  8  0 12]\n"
     ]
    }
   ],
   "source": [
    "print(final_tta_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (tst_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "pred_df.to_csv(\"submission/pred13.csv\", index=False)\n",
    "print(\"Submission file created!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
